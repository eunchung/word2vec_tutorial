{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec tutorial\n",
    "====\n",
    "\n",
    "1. 간단한 이론적 소개\n",
    "2. word2vec 설치\n",
    "3. 영어 코퍼스로 맛보기\n",
    "4. gensim으로 요리하기\n",
    "5. KoNLPy 및 mecab-ko 설치\n",
    "6. 한국어 코퍼스로 word2vec 생성\n",
    "7. 의미 코퍼스로 학습 집합 생성\n",
    "8. 단어 의미 중의성 해결 - Part 1 - 전통적인 방법\n",
    "9. 단어 의미 중의성 해결 - Part 2 - word2vec 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 간단한 이론적 소개\n",
    "----\n",
    "\n",
    "### Efficient Estimation of Word Representations in Vector Space\n",
    "* 논문: http://arxiv.org/pdf/1301.3781.pdf\n",
    "* 발표 자료: https://drive.google.com/file/d/0B7XkCwpI5KDYRWRnd1RzWXQ2TWc/edit?usp=sharing\n",
    "\n",
    "### Backgrounds(Prerequisite)\n",
    "* Perceptron: http://en.wikipedia.org/wiki/Perceptron\n",
    "* Artificial Neural Network(ANN): http://en.wikipedia.org/wiki/Artificial_neural_network\n",
    "  - Back-propagation: http://en.wikipedia.org/wiki/Backpropagation\n",
    "* Deep Neural Network(DNN): http://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks\n",
    "  - feed-foward NN: http://en.wikipedia.org/wiki/Feedforward_neural_network\n",
    "  - Recurrent NN(RNN): http://en.wikipedia.org/wiki/Recurrent_neural_network\n",
    "  - feed-forward vs recurrent: http://stats.stackexchange.com/questions/2213/feed-forward-and-recurrent-neural-networks\n",
    "\n",
    "### Text(word) Representation\n",
    "* Local representation\n",
    "  - n-grams\n",
    "  - bag-of-words\n",
    "  - 1-of-N coding: http://en.wikipedia.org/wiki/Constant-weight_code#1_of_N_codes\n",
    "* Continuous representation\n",
    "  - Latent Semantic Index(LSA)\n",
    "  - Latent Dirichlet Allocation(LDA)\n",
    "  - Distributed representation\n",
    "\n",
    "### Training complexity(O)\n",
    "* O = E x T x Q\n",
    "  - E: number of the training epochs (iteration, 3 ~ 50)\n",
    "  - T: number of the words in the training set (~ 1 billion)\n",
    "  - Q: defined further for each model architecture\n",
    "\n",
    "### Previous works\n",
    "#### feed-forward Neural Net Language Model(NNLM)\n",
    "![](img/feedforwardNNLM.png)\n",
    "* predicts current word by previous n words\n",
    "* the word vectors are in matrix U\n",
    "* trained by stochastic gradient descent and back-propagation\n",
    "* complexity\n",
    "  - Q = N x D + N x D x H + H x V\n",
    "    * N: n previous words (n-gram,  ~ 10)\n",
    "    * D: size of projection layer (dimension, 500 ~ 1000)\n",
    "    * H: size of hidden layer\n",
    "    * V: size of vocabulary\n",
    "  - dominant term: N x D x H => very complex\n",
    "  - for efficient learning\n",
    "    * hierarchical softmax: V => log2(V)\n",
    "    * ___negative sampling___\n",
    "      - word2vec explained: http://arxiv.org/abs/1402.3722\n",
    "\n",
    "#### Recurrent Neural Net Language Model(RNNLM)\n",
    "![](img/rnnlm.png)\n",
    "* does not have projection layer; only input, hidden and output layer\n",
    "* recurrent matrix that connects hidden layer to itself, using time-delayed connections\n",
    "* complexity\n",
    "  - Q = H x H + H x V\n",
    "  - dominant term: H x H\n",
    "* http://research.microsoft.com/pubs/189726/rvecs.pdf\n",
    "\n",
    "### New Log-linear Models\n",
    "#### Continuous Bag-of-Words(CBOW) model\n",
    "![](img/cbow.png)\n",
    "* predicts the current word given the context\n",
    "* ___the non-linear hidden layer is removed___ and the projection layer is shared for all words\n",
    "* the order of words in the history does not influence the projection\n",
    "* complexity\n",
    "  - Q = N x D + D x log2(V)\n",
    "\n",
    "#### Continuous skip-gram model\n",
    "![](img/skip-gram.png)\n",
    "* predicts the surrounding words given the current word\n",
    "* tries to maximize classification of a word based on another word in the same sentence\n",
    "* complexity\n",
    "  - Q = C x (D + D x log2(V))\n",
    "    * C: maximum distance of the words (window size, 10)\n",
    "\n",
    "### Experiments\n",
    "#### Measure\n",
    "* Linguistic regularities\n",
    "  - \"What is the word that is similar to 'small' in in the same sense as 'biggest' is similar to 'big'?\"\n",
    "    * compute vector X = vector('biggest') - vector('big') + vector('small')\n",
    "    * closest vector from X is vector('smallest')\n",
    "* semantic and syntactic regularities\n",
    "![](img/linguistic-regularities.png)\n",
    "* calcuates the accuracy\n",
    "\n",
    "#### Evaluation set\n",
    "* Semantic-Syntactic Word Relationship test set (20K questions)\n",
    "![](img/semantic-syntactic-types.png)\n",
    "* The Microsoft Research Sentence Completion Challenge\n",
    "  - http://research.microsoft.com/apps/pubs/default.aspx?id=157031\n",
    "\n",
    "#### Results\n",
    "* 20K questions set\n",
    "![](img/results.png)\n",
    "* Microsoft set\n",
    "![](img/microsoft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. word2vec 설치\n",
    "----\n",
    "\n",
    "### 다운로드\n",
    "\n",
    "```\n",
    "$ svn checkout http://word2vec.googlecode.com/svn/trunk/ word2vec\n",
    "```\n",
    "\n",
    "### 빌드\n",
    "#### Linux\n",
    "\n",
    "```\n",
    "$ cd word2vec\n",
    "$ make\n",
    "```\n",
    "\n",
    "#### Mac\n",
    "* makefile 내용 수정\n",
    "\n",
    "```makefile\n",
    "# CFLAGS = -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result\n",
    "CFLAGS = -lm -pthread -O3 -Wall -funroll-loops    # -march=native, -Wno-unused-result 제거\n",
    "```\n",
    "\n",
    "* 소스코드 수정\n",
    "\n",
    "```\n",
    "$ grep malloc.h *.c\n",
    "compute-accuracy.c:#include <malloc.h>\n",
    "distance.c:#include <malloc.h>\n",
    "word-analogy.c:#include <malloc.h>\n",
    "```\n",
    "malloc.h를 include하는 코드를 모두 지워버립니다. 혹시 아까우시다면 주석 처리를 해도 괜찮아요. ^^;\n",
    "\n",
    "\n",
    "* 빌드\n",
    "\n",
    "```\n",
    "$ make\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 영어 코퍼스로 맛보기\n",
    "----\n",
    "\n",
    "* 샘플 코퍼스 다운로드\n",
    "```\n",
    "$ wget http://mattmahoney.net/dc/text8.zip\n",
    "$ unzip text8.zip\n",
    "```\n",
    "\n",
    "* word vector 생성\n",
    "\n",
    "```\n",
    "$ ./word2vec -train text8 -output text8.w2v -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 4 -binary 1 -iter 15\n",
    "Starting training using file text8\n",
    "Vocab size: 71291\n",
    "Words in train file: 16718843\n",
    "Alpha: 0.000005  Progress: 100.02%  Words/thread/sec: 92.08k\n",
    "\n",
    "$ ls -l text8.w2v\n",
    "-rw-r--r--  1 jamie  staff  57704809 11  6 01:12 text8.w2v\n",
    "```\n",
    "-threads argument는 CPU 코어의 갯수에 맞게 적절히 넣으면 됩니다.\n",
    "\n",
    "\n",
    "* word analogy 해보기\n",
    "\n",
    "A B C 세 단어를 (소문자로) 넣으면 vector(A) - vector(B) + vector(C)를 계산하여 벡터간 거리로 정렬하여 가장 가까운 단어부터 보여줍니다. (우왕ㅋ굳ㅋ)\n",
    "\n",
    "```\n",
    "$ ./word-analogy text8.w2v\n",
    "Enter three words (EXIT to break): athens greece baghdad\n",
    "\n",
    "Word: athens  Position in vocabulary: 3068\n",
    "\n",
    "Word: greece  Position in vocabulary: 1248\n",
    "\n",
    "Word: baghdad  Position in vocabulary: 7173\n",
    "\n",
    "                                              Word              Distance\n",
    "------------------------------------------------------------------------\n",
    "                                              iran\t\t0.490723\n",
    "                                             syria\t\t0.488516\n",
    "                                             egypt\t\t0.458451\n",
    "                                            turkey\t\t0.450124\n",
    "                                             basra\t\t0.448245\n",
    "                                              iraq\t\t0.444451\n",
    "                                        sassanians\t\t0.438153\n",
    "                                             mosul\t\t0.431619\n",
    "                                           lebanon\t\t0.428784\n",
    "                                         kurdistan\t\t0.426602\n",
    "                                              sadr\t\t0.423816\n",
    "                                            persia\t\t0.423356\n",
    "                                             konya\t\t0.421188\n",
    "                                           abbasid\t\t0.415041\n",
    "                                            khitan\t\t0.410358\n",
    "                                           bukhara\t\t0.410169\n",
    "                                            kirkuk\t\t0.405520\n",
    "                                           fatimid\t\t0.404706\n",
    "                                           iranian\t\t0.404393\n",
    "                                           ayyubid\t\t0.404103\n",
    "                                           mameluk\t\t0.403826\n",
    "                                             libya\t\t0.400661\n",
    "                                           almohad\t\t0.400090\n",
    "                                          pakistan\t\t0.398301\n",
    "                                          iranians\t\t0.397244\n",
    "                                              arab\t\t0.395184\n",
    "                                              zmir\t\t0.394183\n",
    "                                            zagros\t\t0.394126\n",
    "                                            mashal\t\t0.393248\n",
    "                                             balkh\t\t0.393002\n",
    "                                       afghanistan\t\t0.391483\n",
    "                                           tunisia\t\t0.389862\n",
    "                                           tahmasp\t\t0.389630\n",
    "                                        uzbekistan\t\t0.389564\n",
    "                                            tigris\t\t0.389527\n",
    "                                        tajikistan\t\t0.389425\n",
    "                                           emirate\t\t0.388417\n",
    "                                        kermanshah\t\t0.388197\n",
    "                                           khanate\t\t0.385721\n",
    "                                             hejaz\t\t0.384893\n",
    "```\n",
    "\n",
    "* 샘플 word analogy 쿼리\n",
    "\n",
    "```\n",
    "$ grep -A1 \"^:\" questions-words.txt\n",
    ": capital-common-countries\n",
    "Athens Greece Baghdad Iraq\n",
    "--\n",
    ": capital-world\n",
    "Abuja Nigeria Accra Ghana\n",
    "--\n",
    ": currency\n",
    "Algeria dinar Angola kwanza\n",
    "--\n",
    ": city-in-state\n",
    "Chicago Illinois Houston Texas\n",
    "--\n",
    ": family\n",
    "boy girl brother sister\n",
    "--\n",
    ": gram1-adjective-to-adverb\n",
    "amazing amazingly apparent apparently\n",
    "--\n",
    ": gram2-opposite\n",
    "acceptable unacceptable aware unaware\n",
    "--\n",
    ": gram3-comparative\n",
    "bad worse big bigger\n",
    "--\n",
    ": gram4-superlative\n",
    "bad worst big biggest\n",
    "--\n",
    ": gram5-present-participle\n",
    "code coding dance dancing\n",
    "--\n",
    ": gram6-nationality-adjective\n",
    "Albania Albanian Argentina Argentinean\n",
    "--\n",
    ": gram7-past-tense\n",
    "dancing danced decreasing decreased\n",
    "--\n",
    ": gram8-plural\n",
    "banana bananas bird birds\n",
    "--\n",
    ": gram9-plural-verbs\n",
    "decrease decreases describe describes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. gensim으로 요리하기\n",
    "----\n",
    "\n",
    "\n",
    "### gensim 설치\n",
    "\n",
    "```\n",
    "$ pip install gensim\n",
    "```\n",
    "끝 (우왕ㅋ굳ㅋ)\n",
    "\n",
    "* 참고\n",
    "  - https://radimrehurek.com/gensim/models/word2vec.html\n",
    "  - http://rare-technologies.com/word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### word2vec 모델을 만들어 봅시다.\n",
    "\n",
    "import gensim\n",
    "\n",
    "sentences = sum([line.split() for line in open('text8')], [])    # text8 코퍼스를 모두 읽어들입니다.\n",
    "print sentences[:10]\n",
    "\n",
    "# C 프로그램과 유사하게 학습을 실행합니다.\n",
    "gensim_model = gensim.models.Word2Vec(sentences, size=200, window=8, min_count=5, workers=4, iter=15)\n",
    "\n",
    "gensim_model.save('text8.w2v.gensim')    # 학습한 모델을 저장합니다.\n",
    "gensim_new_model = gensim.models.Word2Vec.load('text8.w2v.gensim')    # 저장한 모델을 읽어들입니다.\n",
    "\n",
    "# 하지만 오래 살기 위해 이러지 말고 Jeff Dean을 믿고 씁시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://static1.businessinsider.com/image/51098904ecad04ad07000027/meet-googles-baddest-engineer-jeff-dean.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* woman - man + king = [(u'queen', 0.5841079354286194), (u'betrothed', 0.5011240243911743), (u'throne', 0.48706796765327454)]\n",
      "\n",
      "* The lone duckling = cereal\n",
      "\n",
      "* Similarity between woman and man = 0.599900135036\n",
      "\n",
      "* Vector of man:\n",
      "[-0.02120703 -0.05611495 -0.0448945  -0.10382977  0.06347878 -0.03084225\n",
      " -0.05465495 -0.06092014  0.0411589  -0.01161192 -0.03177946 -0.00450953\n",
      "  0.08355674  0.08220534  0.09392931  0.07430327 -0.09978093  0.0255994\n",
      "  0.06163046 -0.00767307  0.0402733  -0.0438408  -0.09186259 -0.070136\n",
      " -0.13232407  0.01343085  0.02881281 -0.01540345  0.10018467  0.02999429\n",
      " -0.04141324  0.03193626  0.04727618  0.03440831 -0.05067592 -0.13143836\n",
      "  0.04143735  0.08112337  0.09542105 -0.01772195  0.07869302  0.03986564\n",
      " -0.03947217 -0.04002963  0.12870036 -0.00388008 -0.11321796  0.03314336\n",
      "  0.08151811  0.12611714  0.10488386  0.026107   -0.0391846   0.07439879\n",
      "  0.04260764 -0.08983206  0.06683791  0.05873458  0.06682455  0.06476297\n",
      "  0.14592303  0.0288553   0.02531698 -0.14798169 -0.07183906  0.04604707\n",
      "  0.05763122  0.00544797  0.01079247  0.02088901  0.09337061  0.02174541\n",
      "  0.09076272 -0.12938984  0.12643127 -0.06724159 -0.10788886 -0.06445206\n",
      " -0.01494339 -0.05210608  0.09026874 -0.01870122 -0.07760402 -0.03570227\n",
      "  0.03812196  0.05874873 -0.03259608 -0.10213429 -0.04451574 -0.00774511\n",
      " -0.03484794 -0.04241935  0.01966534 -0.0786922  -0.07696231  0.01421919\n",
      " -0.06037491 -0.07596584 -0.07149822 -0.17544015 -0.03485671  0.04678441\n",
      "  0.09777086  0.01886008  0.1079479  -0.01725644  0.05858152 -0.0092585\n",
      "  0.07580883  0.04431532  0.03641187  0.07590564  0.00328455 -0.19461598\n",
      "  0.02172673  0.0468547   0.06359043 -0.06067419  0.07218818 -0.06806297\n",
      " -0.00102148 -0.05882948 -0.09755743  0.00437786  0.09942376 -0.01708928\n",
      "  0.00382364  0.08331228 -0.01728678  0.14367285 -0.08850611  0.04921585\n",
      "  0.04936095 -0.03093628 -0.11593633 -0.01380507  0.06000147 -0.01196247\n",
      "  0.04775746  0.10399673 -0.04833509  0.05022232 -0.03844653 -0.03018727\n",
      "  0.13787274  0.04292813  0.06317206 -0.00937788  0.07239998  0.14757171\n",
      " -0.10221025 -0.07186496 -0.08656952  0.04150686 -0.00996734 -0.04674741\n",
      "  0.03688774 -0.00214836  0.02067067 -0.01934814  0.02112807 -0.15846916\n",
      " -0.05862749  0.06297677  0.11002935 -0.05991796 -0.04351886 -0.00706006\n",
      " -0.06823254  0.03268349  0.05168799 -0.01327381  0.08289593  0.23329559\n",
      "  0.04977767  0.10524985 -0.00270658  0.07449356  0.05844907 -0.11847495\n",
      "  0.03599216 -0.02173296 -0.06586666  0.04696305  0.05606075 -0.05920108\n",
      "  0.05524338 -0.081884   -0.00758805 -0.03253384  0.01980517 -0.06057641\n",
      " -0.03342021  0.01647547  0.03814693 -0.01475905 -0.08263606 -0.06316575\n",
      " -0.11919118  0.04565263]\n"
     ]
    }
   ],
   "source": [
    "### 기존에 C 프로그램으로 학습한 모델을 읽어들여 여러가지 연산을 해봅니다.\n",
    "\n",
    "import gensim\n",
    "\n",
    "gensim_model = gensim.models.Word2Vec.load_word2vec_format('text8.w2v', binary=True)    # 모델을 읽어들입니다.\n",
    "\n",
    "# word analogy\n",
    "print '* woman - man + king =', gensim_model.most_similar(positive=[u'woman', u'king'], negative=[u'man'], topn=3)\n",
    "print\n",
    "\n",
    "# 미운 오리 새끼를 찾습니다. ;)\n",
    "print '* The lone duckling =', gensim_model.doesnt_match(u'breakfast cereal dinner lunch'.split())\n",
    "print\n",
    "\n",
    "# 두 단어의 유사도를 계산합니다.\n",
    "print '* Similarity between woman and man =', gensim_model.similarity(u'woman', u'man')\n",
    "print\n",
    "\n",
    "# 단어의 벡터를 출력합니다.\n",
    "print '* Vector of man:'\n",
    "print gensim_model[u'man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. KoNLPy 및 mecab-ko 설치\n",
    "----\n",
    "\n",
    "### konlpy 패키지 설치\n",
    "\n",
    "```\n",
    "$ pip install konlpy\n",
    "```\n",
    "\n",
    "### mecab-ko 프로그램 설치\n",
    "\n",
    "* 아래와 같은 마법의 스크립트는 에러가 나니 한땀한땀 해보겠습니다.\n",
    "\n",
    "```\n",
    "$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "```\n",
    "\n",
    "* \\$HOME/usr/mecab-ko 아래에 설치한다고 가정하고 아래와 같이 소스코드를 다운로드합니다.\n",
    "\n",
    "```\n",
    "$ cd $HOME/usr\n",
    "$ wget https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
    "```\n",
    "\n",
    "* 압축을 풀고 빌드/설치를 합니다.\n",
    "\n",
    "```\n",
    "$ tar xfz mecab-0.996-ko-0.9.2.tar.gz\n",
    "$ cd mecab-0.996-ko-0.9.2\n",
    "$ ./configure --prefix=$HOME/usr/mecab-ko\n",
    "$ make && make install\n",
    "```\n",
    "\n",
    "### mecab-ko 사전 설치\n",
    "\n",
    "* 역시 \\$HOME/usr/mecab-ko 아래에 설치한다고 가정하고 아래와 같이 사전을 다운로드합니다.\n",
    "\n",
    "```\n",
    "$ cd $HOME/usr\n",
    "$ wget https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz\n",
    "```\n",
    "\n",
    "* 압축을 풀고 빌드/설치를 합니다.\n",
    "\n",
    "```\n",
    "$ tar xfz mecab-ko-dic-2.0.1-20150920.tar.gz\n",
    "$ cd mecab-ko-dic-2.0.1-20150920\n",
    "$ ./autogen.sh\n",
    "$ ./configure --prefix=$HOME/usr/mecab-ko --with-mecab-config=$HOME/usr/mecab-ko/bin/mecab-config\n",
    "$ make && make install\n",
    "```\n",
    "\n",
    "### 환경 설정\n",
    "\n",
    "```\n",
    "$ export LD_LIBRARY_PATH=$HOME/usr/mecab-ko/lib:$LD_LIBRARY_PATH    # Linux의 경우\n",
    "$ export DYLD_LIBRARY_PATH=$HOME/usr/mecab-ko/lib:$DYLD_LIBRARY_PATH    # Mac의 경우\n",
    "```\n",
    "\n",
    "### 테스트\n",
    "\n",
    "```\n",
    "$ cd $HOME/usr/mecab-ko\n",
    "$ bin/mecab\n",
    "존나좋군?\n",
    "존나\tMAG,*,F,존나,*,*,*,*\n",
    "좋\tVA,*,T,좋,*,*,*,*\n",
    "군\tEF,*,T,군,*,*,*,*\n",
    "?\tSF,*,*,*,*,*,*,*\n",
    "EOS\n",
    "```\n",
    "![](http://pds23.egloos.com/pds/201112/24/41/d0021441_4ef5e7b6eb457.jpg)\n",
    "\n",
    "### mecab-ko python 모듈 설치\n",
    "\n",
    "* 소스 코드를 다운로드 합니다.\n",
    "\n",
    "```\n",
    "$ cd $HOME/usr\n",
    "$ git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
    "```\n",
    "\n",
    "* 설치 및 빌드를 합니다.\n",
    "\n",
    "```\n",
    "$ cd mecab-python-0.996\n",
    "$ python setup.py build\n",
    "$ sudo python setup.py install    # virtualenv를 사용하신다면 sudo는 내려놓으셔도 됩니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(존나, MAG),\n",
      " (좋, VA),\n",
      " (군, EF),\n",
      " (?, SF)]\n"
     ]
    }
   ],
   "source": [
    "### KoNLPy에서 mecab-ko를 사용해 봅시다.\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "import os\n",
    "\n",
    "# 사전 경로를 주지 않으면 /usr/local/lib/mecab/dic/mecab-ko-dic을 찾습니다.\n",
    "mecab = Mecab('%s/usr/mecab-ko/lib/mecab/dic/mecab-ko-dic' % os.environ['HOME'])\n",
    "pprint(mecab.pos(u'존나좋군?'))    # [(형태소1, 태그1), (형태소2, 태그2), ...)] 튜플의 리스트 형태로 리턴합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 한국어 코퍼스로 word2vec 생성\n",
    "----\n",
    "\n",
    "### 원시 코퍼스\n",
    "\n",
    "* 원시 코퍼스의 압축을 푼다.\n",
    "\n",
    "```\n",
    "$ cat raw_corpus.zip.0 raw_corpus.zip.1 >> raw_corpus.zip\n",
    "$ unzip -q raw_corpus.zip\n",
    "```\n",
    "\n",
    "* 인코딩을 UTF-16에서 UTF-8로 변환하여 하나의 파일로 합친다. => raw_corpus.xml\n",
    "\n",
    "```\n",
    "$ iconv -f UTF-16 -t UTF-8 raw_corpus/*.txt > raw_corpus.xml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"정치 신뢰회복 최선 다하겠다\" / 박태준 민정대표 일문일답\r\n",
      "\"당내 융화-결속엔 별 문제 없어 / 민주화 촉진-경제난 극복에 맞춰 당운영\"\r\n",
      "민정당의 신임 박태준대표위원은 \"기왕에 정치일선에 나선이상 신명을 바쳐 저하된 정치의 신뢰를 회복하는 데 최선을 다하겠다\"고 첫 소감을 말했다.\r\n",
      "5일 오후 노태우대통령으로부터 대표위원 임명통보를 받은 뒤 오후 4 시쯤 민정당사에 도착, 곧바로 가진 기자회견에서 그는 밝은 표정으로 질문에 응했으나 정계개편등 중요 현안에 대해서는 \"앞으로 공부를 더해 답하겠다\"는 말로 답변을 대신했다.\r\n",
      "- 어려운 시기에 대표위원을 맡았는데….\r\n"
     ]
    }
   ],
   "source": [
    "### 태그를 제거하고 문장만 남긴다. => raw_corpus.txt\n",
    "\n",
    "def strip_tag(line, tag):\n",
    "    \"\"\"\n",
    "    라인을 둘러싼 열고 닫는 xml 태그를 제거하는 함수\n",
    "    @param  line  xml 태그를 포함하는 라인\n",
    "    @param  tag   태그명\n",
    "    @return       태그가 제거된 라인\n",
    "    \"\"\"\n",
    "    open_tag = '<%s>' % tag\n",
    "    close_tag = '</%s>' % tag\n",
    "    stripped = line\n",
    "    if stripped.startswith(open_tag):\n",
    "        stripped = stripped[len(open_tag):]\n",
    "    if stripped.endswith(close_tag):\n",
    "        stripped = stripped[:-len(close_tag)]\n",
    "    return stripped.strip()\n",
    "\n",
    "with open('raw_corpus.txt', 'w') as fout:\n",
    "    for line in open('raw_corpus.xml'):    # 앞서 xml로 저장한 원시 코퍼스를 열어서\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith('<head>') or line.startswith('<p>'):    # <head>, <p> 태그로 시작하는 문장만\n",
    "            line = strip_tag(line, 'head')\n",
    "            line = strip_tag(line, 'p')\n",
    "            if line:\n",
    "                print >> fout, line    # 한 줄에 한 문장씩 출력\n",
    "\n",
    "!head -n5 raw_corpus.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"/SY 정치/NNG 신뢰/NNG 회복/NNG 최선/NNG 다/MAG 하/VV 겠/EP 다/EC \"/SY //SC 박태준/NNP 민정/NNG 대표/NNG 일문일답/NNG\r\n",
      "\"/SY 당내/NNG 융화/NNG -/SY 결속/NNG 엔/JKB+JX 별/MM 문제/NNG 없/VA 어/EF //SC 민주/NNG 화/XSN 촉진/NNG -/SY 경제난/NNG 극복/NNG 에/JKB 맞춰/VV+EC 당/NNG 운영/NNG \"/SY\r\n",
      "민/NNP 정당/NNG 의/JKG 신임/NNG 박태준/NNP 대표/NNG 위원/NNG 은/JX \"/SY 기왕/NNG 에/JKB 정치/NNG 일선/NNG 에/JKB 나선/VV+ETM 이상/NNG 신명/NNG 을/JKO 바쳐/VV+EC 저하/NNG 된/XSV+ETM 정치/NNG 의/JKG 신뢰/NNG 를/JKO 회복/NNG 하/XSV 는/ETM 데/NNB 최선/NNG 을/JKO 다/MAG 하/VV 겠/EP 다/EC \"/SY 고/JKQ 첫/MM 소감/NNG 을/JKO 말/NNG 했/XSV+EP 다/EF ./SF\r\n",
      "5/SN 일/NNBC 오후/NNG 노태우/NNP 대통령/NNG 으로부터/JKB 대표/NNG 위원/NNG 임명/NNG 통보/NNG 를/JKO 받/VV 은/ETM 뒤/NNG 오후/NNG 4/SN 시/NNBC 쯤/XSN 민정/NNG 당사/NNG 에/JKB 도착/NNG ,/SC 곧바로/MAG 가진/VV+ETM 기자/NNG 회견/NNG 에서/JKB 그/NP 는/JX 밝/VA 은/ETM 표정/NNG 으로/JKB 질문/NNG 에/JKB 응했/VV+EP 으나/EC 정계/NNG 개편/NNG 등/NNB 중요/NNG 현안/NNG 에/JKB 대해서/VV+EC 는/JX \"/SY 앞/NNG 으로/JKB 공부/NNG 를/JKO 더해/VV+EC 답/NNG 하/XSV 겠/EP 다/EC \"/SY 는/JX 말/NNG 로/JKB 답변/NNG 을/JKO 대신/NNG 했/XSV+EP 다/EF ./SF\r\n",
      "-/SY 어려운/VA+ETM 시기/NNG 에/JKB 대표/NNG 위원/NNG 을/JKO 맡/VV 았/EP 는데/EC …/SE ./SF\r\n"
     ]
    }
   ],
   "source": [
    "### KoNLPy를 이용해 품사 태깅을 수행한다. => raw_corpus.tag\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "import os\n",
    "\n",
    "mecab = Mecab('%s/usr/mecab-ko/lib/mecab/dic/mecab-ko-dic' % os.environ['HOME'])\n",
    "\n",
    "with open('raw_corpus.tag', 'w') as fout:\n",
    "    for line in open('raw_corpus.txt'):    # 앞서 txt로 저장한 원시 코퍼스를 열어서\n",
    "        for morph, tag in mecab.pos(unicode(line, 'UTF-8')):    # 형태소 분석을 수행한 다음\n",
    "            print >> fout, '%s/%s' % (morph.encode('UTF-8'), tag.encode('UTF-8')),    # '정치/NNG'의 형태로\n",
    "        print >> fout    # 한 줄에 한 문장씩 출력\n",
    "\n",
    "!head -n5 raw_corpus.tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앞서 생성한 자동 태깅한 코퍼스를 이용하여 '단어/태그'의 벡터를 생성한다. => raw_corpus.w2v\n",
    "\n",
    "```\n",
    "$ ./word2vec -train raw_corpus.tag -output raw_corpus.w2v -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 4 -binary 1 -iter 15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 의미 코퍼스로 학습 집합 생성\n",
    "----\n",
    "\n",
    "### 의미 코퍼스\n",
    "\n",
    "* 의미 코퍼스의 압축을 푼다.\n",
    "\n",
    "```\n",
    "$ cat sense_corpus.zip.0 sense_corpus.zip.1 >> sense_corpus.zip\n",
    "$ unzip -q sense_corpus.zip\n",
    "```\n",
    "\n",
    "* 인코딩을 UTF-16에서 UTF-8로 변환하여 하나의 파일로 합친다. => sense_corpus.xml\n",
    "\n",
    "```\n",
    "$ iconv -f UTF-16 -t UTF-8 sense_corpus/*.txt > sense_corpus.xml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\t일본/NNP 의/JKG 경우/NNG 현재/MAG 주택/NNG 내부/NNG 에/JKB 설치/NNG 하/XSV 는/ETM 열/NNG 감지/NNG 형/XSN 화재/NNG 감지기/NNG 는/JX 18/SN 년/NNB ,/SP 아파트/NNG 의/JKG 복도/NNG -/SS 계단/NNG 등/NNB 에/JKB 설치/NNG 하/XSV 는/ETM 연기/NNG 감지/NNG 형/XSN 화재/NNG 감지기/NNG 10/SN 년/NNB 으로/JKB 사용/NNG 기간/NNG 을/JKO 못/NNG 박/VV 고/EC 있/VX 다/EF ./SF\r\n",
      "9\t또/MAG 최근/NNG 캐나다/NNP 의/JKG 소비자/NNG 잡지/NNG '/SS 캐네디언/NNG 컨슈머/NNG '/SS 는/JX 연기/NNG 탐지기/NNG 의/JKG 경우/NNG 수명/NNG 이/JKS 5/SN ∼/SO 10/SN 년/NNB 이/VCP 고/EC ,/SP 전체/NNG 의/JKG 20/SN ∼/SO 30/SN %/SW 는/JX 10/SN 년/NNB 내/NNB 기능/NNG 이/JKS 멈추/VV ᆫ다고/EC 지적/NNG ,/SP 눈길/NNG 을/JKO 끌/VV ᆫ다/EF ./SF\r\n",
      "9\t연기/NNG 감지/NNG 형/XSN 이/VCP ᆫ/ETM 경우/NNG 제사/NNG 때/NNG 쓰/VV 는/ETM 향/NNG 3/SN 개/NNB 정도/NNG 를/JKO 한꺼번에/MAG 피우/VV 어/EC 경보기/NNG 에/JKB 갖/VV 다/EC 대/VX 면/EC 소리/NNG 가/JKS 나/VV ᆫ다/EF ./SF\r\n",
      "5\t서/NNP 청장/NNG 은/JX 특히/MAG 법인세/NNG 세무/NNG 조사/NNG 대상/NNG 기업/NNG 을/JKO 현재/NNG 의/JKG 두/MM 배/NNG 로/JKB 늘리/VV 고/EC 현재/MAG 60/SN 일/NNB 이/VCP 던/ETM 각/MM 기업/NNG 의/JKG 조사/NNG 기한/NNG 을/JKO 각/MM 지방/NNG 청장/NNG 재량/NNG 에/JKB 따르/VV 아/EC 필요/NNG 하/XSA 다고/EC 판단/NNG 하/XSV 았/EP 을/ETM 때/NNG 연기/NNG 하/XSV ᆯ/ETM 수/NNB 있/VV 도록/EC 하/VX 겠/EP 다고/EC 밝히/VV 었/EP 다/EF ./SF\r\n",
      "5\t또/MAG 어렵/VA ᆫ/ETM 여건/NNG 에/JKB 처하/VV ᆫ/ETM 기업/NNG 의/JKG 생산/NNG 활동/NNG 을/JKO 뒷받침/NNG 하/XSV ᆫ다는/ETM 방침/NNG 에/JKB 따르/VV 아/EC 설비/NNG 확장/NNG 이나/JC 기술/NNG 개발/NNG 등/NNB 에/JKB 주력/NNG 하/XSV 는/ETM 제조/NNG 업체/NNG 는/JX 신고/NNG 내용/NNG 이/JKS 다소/MAG 부실/NNG 하/VV 아도/EC 조사/NNG 를/JKO 하/VV 지/EC 않/VX 기/ETN 로/JKB 하/VX 았/EP 으며/EC 조사/NNG 대상/NNG 에/JKB 포함/NNG 되/XSV ᆯ/ETM 경우/NNG 에/JKB 도/JX 하반기/NNG 이후/NNG 로/JKB 실시/NNG 를/JKO 연기/NNG 하/XSV 기/ETN 로/JKB 하/VX 았/EP 다/EF ./SF\r\n"
     ]
    }
   ],
   "source": [
    "### 특정 단어를 포함하는 문장을 추출하여 단어의 의미 번호와 함께 출력한다. => sense_corpus.tag\n",
    "\n",
    "TARGET = '연기__'    # 추출을 원하는 단어. 예: 배, 경기, 연기\n",
    "\n",
    "import re\n",
    "\n",
    "LINE_ID_PTN = re.compile('[0-9A-Z_]{4}\\\\d{4}-\\\\d{7,8}')    # 형태소 분석 된 어절 한 줄의 맨 앞에 ID 패턴\n",
    "\n",
    "with open('sense_corpus.tag', 'w') as fout:\n",
    "    sent = []\n",
    "    for line in open('sense_corpus.xml'):    # 앞서 xml로 저장한 의미 코퍼스를 열어서\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if LINE_ID_PTN.match(line):    # 다른 라인은 버리고 어절만을 취하여\n",
    "            _, morphs = line.rsplit('\\t', 1)\n",
    "            sent.extend(morphs.split(' + '))\n",
    "        else:\n",
    "            # 한 문장에서 원하는 단어의 의미 번호를 모두 추출하여\n",
    "            sense_nums = set([morph[len(TARGET):morph.rindex('/')] for morph in sent if morph.startswith(TARGET)])\n",
    "            if len(sense_nums) == 1:    # 한 문장에 원하는 단어의 의미가 한가지로만 쓰인 문장만을 골라서\n",
    "                try:\n",
    "                    sense_num = int(list(sense_nums)[0])\n",
    "                    if sense_num < 50:    # 의미 번호가 너무 크면 뭔가 에러가 있으므로 버리고\n",
    "                        # 단어에 붙어있는 모든 의미 번호를 제거하고\n",
    "                        no_sense_tag = [re.sub(r\"\"\"__x?\\d\\d/\"\"\", '/', morph) for morph in sent]\n",
    "                        # 문장 맨 앞에 원하는 단어의 의미 번호를 함께 출력\n",
    "                        print >> fout, '%d\\t%s' % (sense_num, ' '.join(no_sense_tag))\n",
    "                except ValueError:\n",
    "                    # 간혹 의미 번호가 '연기__x01'과 같이 특수하게 부여된 것이 있는데, 이런건 버린다.\n",
    "                    pass\n",
    "            sent = []\n",
    "\n",
    "!head -n5 sense_corpus.tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "8. 단어 의미 중의성 해결 - Part 1 - 전통적인 방법\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 의미 코퍼스를 읽어들입니다.\n",
    "\n",
    "sense_corpus = []\n",
    "for line in open('sense_corpus.tag'):\n",
    "    sense, features = line.split('\\t', 1)\n",
    "    sense_corpus.append((int(sense), features.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabulary: 7628\n",
      "First training instance:\n",
      "(9, array([0, 0, 0, ..., 0, 0, 0], dtype=int8))\n"
     ]
    }
   ],
   "source": [
    "### 전부 바이너리 벡터로 표현합니다.\n",
    "\n",
    "import numpy as npy\n",
    "\n",
    "def to_bin_vector(vocabulary, features, trg):\n",
    "    \"\"\"\n",
    "    vocabulary 사전을 이용하여 모든 자질을 하나의 bag-of-words 벡터로 표현하는 함수\n",
    "    @param  vocabulary  vocabulary 사전\n",
    "    @param  features    자질 목록\n",
    "    @param  trg         중의성을 해소할 단어\n",
    "    @return             벡터\n",
    "    \"\"\"\n",
    "    vec_sum = npy.zeros([len(vocabulary),], dtype=npy.int8)    # vocabulary 크기 만큼 벡터를 0으로 초기화 합니다.\n",
    "    for feature in features:\n",
    "        if feature.startswith('%s/' % trg):\n",
    "            continue\n",
    "        vec_sum[vocabulary[feature]] = 1    # 자질 번호에 해당하는 벡터 값을 1로 설정해 줍니다.\n",
    "    return vec_sum\n",
    "\n",
    "all_features = set(sum([features for _, features in sense_corpus], []))    # 모든 자질을 합칩니다.\n",
    "vocabulary = {feature: num for num, feature in enumerate(all_features)}    # 각 자질에 고유 번호를 부여하여 사전을 구축합니다.\n",
    "print 'Number of vocabulary:', len(vocabulary)\n",
    "\n",
    "# 자질을 전부 벡터로 표현합니다.\n",
    "bin_train = [(sense, to_bin_vector(vocabulary, features, '연기')) for sense, features in sense_corpus]\n",
    "print 'First training instance:'\n",
    "print bin_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### SVM을 이용하여 학습 및 평가를 수행하는 함수를 작성합니다.\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "\n",
    "def evaluate(X, Y):\n",
    "    \"\"\"\n",
    "    SVM을 이용하여 평가를 수행하는 함수\n",
    "    @param  X  자질 벡터\n",
    "    @param  Y  outcome\n",
    "    @return    정확도\n",
    "    \"\"\"\n",
    "    # 학습 코퍼스를 8:2의 비율로 학습/평가 코퍼스로 나눕니다.\n",
    "    X_train, X_eval, y_train, y_eval = cross_validation.train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    svm_model = svm.LinearSVC()\n",
    "    svm_model.fit(X_train, y_train)    # 학습 집합을 이용해 학습합니다.\n",
    "    return svm_model.score(X_eval, y_eval)    # 평가 집합으로 정확도를 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86868686868686873"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 평가를 해봅니다.\n",
    "\n",
    "evaluate([features for _, features in bin_train], [sense for sense, _ in bin_train])\n",
    "\n",
    "# 오!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://icongal.com/gallery/image/38973/thumbs_up_thumbs_up_vote_like.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 단어 의미 중의성 해결 - Part 2 - word2vec 활용\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 200\n",
      "First training instance:\n",
      "(9, array([ 2.03780541, -0.05760974,  0.45845033,  0.16865686,  0.38574068,\n",
      "        0.63337594, -1.1427569 , -0.10811525, -0.01907412, -1.21633177,\n",
      "        0.47648489, -0.04538528,  0.17248241, -0.67216815, -0.58530399,\n",
      "       -0.0795107 ,  0.11482168,  0.30661339,  0.45757701, -0.53109309,\n",
      "       -0.20708619,  0.29574604, -0.84984107,  0.91007282, -0.54807297,\n",
      "        0.86658714, -1.33023983,  0.15370091, -0.76278742, -0.29612444,\n",
      "       -0.05138013,  0.64856726,  0.97502029,  0.17566016,  0.96827262,\n",
      "        0.25050572, -1.01533307, -0.93904025, -0.57080799, -0.16608133,\n",
      "       -0.58569992, -0.29802095,  0.04178477,  1.51128502, -0.10311706,\n",
      "       -0.77958182, -0.19423184,  0.42869712, -0.04144236, -0.1677563 ,\n",
      "        0.81574613, -1.20705579,  0.38499597, -0.38002745,  0.42406251,\n",
      "        0.97285284,  0.49995938,  0.42171961, -1.15597911,  0.01568658,\n",
      "       -0.61849618,  1.00722201,  1.04033946, -0.4688745 , -0.21817498,\n",
      "        1.28075883,  0.46158873,  0.38451856,  0.4327886 , -0.97890061,\n",
      "        1.08400226, -1.01580817, -0.37603575, -0.12506738,  0.70589363,\n",
      "       -0.15936265,  0.77877479,  1.16553275,  0.93484401,  0.35201315,\n",
      "        0.60657413,  0.64563316,  1.11461908, -0.25831091, -1.24739922,\n",
      "        0.39880771,  0.83802047, -0.98949321, -0.29775482, -0.59462891,\n",
      "        0.80808286, -0.32399213, -2.28392936,  0.25798794,  0.10969915,\n",
      "        0.69180732, -0.26235604,  0.54557728, -0.16879321, -0.11227091,\n",
      "        0.66568831,  0.43680404,  0.50044791,  0.87308647, -0.3159402 ,\n",
      "       -0.16197947, -0.56830034,  0.34976865,  0.19205878, -0.50927652,\n",
      "        0.27848252, -0.61742951, -0.36190217, -1.50561612, -0.41331244,\n",
      "       -0.65716787,  0.44958811,  0.54835139,  0.19774409, -0.81836083,\n",
      "        0.15593203, -1.98397599, -0.60777089, -0.8946949 ,  0.9292021 ,\n",
      "       -0.57755399, -1.03778202, -0.32868603,  0.86028334, -0.47365999,\n",
      "        0.5346909 , -0.24691136, -0.62691621, -0.41651139, -0.14870743,\n",
      "       -0.76714203, -0.35082127,  0.48852637, -0.30875797,  0.86899167,\n",
      "       -0.35833584,  1.18583267, -0.05436611,  0.50009832, -1.59323129,\n",
      "        0.16616679, -1.24423964,  0.6260075 ,  0.90489828,  1.05251309,\n",
      "       -0.15672637,  0.56280934, -0.38948422, -0.6014004 ,  1.01611516,\n",
      "       -2.35681756, -0.91602259,  0.03976747,  0.0685423 ,  0.30420568,\n",
      "        0.92593142,  0.24051717,  0.24631331, -0.65824391,  0.75406839,\n",
      "       -0.4442042 , -0.35735415,  0.02452918, -0.60685982,  0.36866948,\n",
      "       -0.52669873, -0.46931607, -0.8316927 , -0.16175267,  0.477615  ,\n",
      "       -0.85156652, -0.14145796,  1.21205847, -0.00945503,  0.43452735,\n",
      "        0.79145779, -0.0700663 , -0.24071598,  0.25989331,  0.202665  ,\n",
      "       -0.85898883, -0.56367487,  0.18296803, -1.03091124,  0.14070791,\n",
      "       -0.31932509, -0.49769462,  0.05583623,  0.15912302,  0.30660352,\n",
      "        0.34636421,  0.27651946, -0.13553483, -0.18066616,  1.33475716]))\n"
     ]
    }
   ],
   "source": [
    "### 전부 word2vec을 이용하여 벡터로 표현합니다.\n",
    "\n",
    "import gensim\n",
    "import numpy as npy\n",
    "\n",
    "def to_w2v_vector(model, features, trg):\n",
    "    \"\"\"\n",
    "    word2vec 모델을 이용하여 모든 자질을 더해서 하나의 벡터로 표현하는 함수\n",
    "    @param  model     word2vec 모델\n",
    "    @param  features  자질 목록\n",
    "    @param  trg       중의성을 해소할 단어\n",
    "    @return           벡터\n",
    "    \"\"\"\n",
    "    vec_sum = npy.zeros([model.vector_size,])    # word2vec 모델의 벡터 크기 만큼 벡터를 0으로 초기화 합니다.\n",
    "    for feature in set(features):\n",
    "        if feature.startswith('%s/' % trg):\n",
    "            continue\n",
    "        try:\n",
    "            vec_sum += model[unicode(feature, 'UTF-8')]\n",
    "        except KeyError:\n",
    "            pass    # Out-Of-Vocabulary 자질들은 그냥 넘어갑니다.\n",
    "    return vec_sum\n",
    "\n",
    "# 원시 코퍼스로 pre-training 해둔 word2vec 모델을 읽어들입니다.\n",
    "w2v_model = gensim.models.Word2Vec.load_word2vec_format('raw_corpus.w2v', binary=True)\n",
    "print 'Vector size:', w2v_model.vector_size\n",
    "\n",
    "# 자질을 전부 벡터로 표현합니다.\n",
    "w2v_train = [(sense, to_w2v_vector(w2v_model, features, '연기')) for sense, features in sense_corpus]\n",
    "print 'First training instance:'\n",
    "print w2v_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91245791245791241"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 평가를 해봅니다.\n",
    "\n",
    "evaluate([features for _, features in w2v_train], [sense for sense, _ in w2v_train])\n",
    "\n",
    "# 오오!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](http://i1.daumcdn.net/thumb/R750x0/?fname=http%3A%2F%2Fcfile8.uf.tistory.com%2Fimage%2F2406FE4054BC833F2FA8F7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
