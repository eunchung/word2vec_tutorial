{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec tutorial\n",
    "====\n",
    "\n",
    "1. 간단한 이론적 소개\n",
    "2. word2vec 설치\n",
    "3. 영어 코퍼스로 맛보기\n",
    "4. gensim으로 요리하기\n",
    "5. KoNLPy 설치\n",
    "6. 한국어 코퍼스로 word2vec 생성\n",
    "7. 의미 코퍼스로 학습 집합 생성\n",
    "8. 단어 의미 중의성 해결 - Part 1 - 전통적인 방법\n",
    "9. 단어 의미 중의성 해결 - Part 2 - word2vec 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 간단한 이론적 소개\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. word2vec 설치\n",
    "----\n",
    "\n",
    "### 다운로드\n",
    "\n",
    "```\n",
    "$ svn checkout http://word2vec.googlecode.com/svn/trunk/ word2vec\n",
    "```\n",
    "\n",
    "### 빌드\n",
    "#### Linux\n",
    "\n",
    "```\n",
    "$ cd word2vec\n",
    "$ make\n",
    "```\n",
    "\n",
    "#### Mac\n",
    "* makefile 내용 수정\n",
    "\n",
    "```makefile\n",
    "# CFLAGS = -lm -pthread -O3 -march=native -Wall -funroll-loops -Wno-unused-result\n",
    "CFLAGS = -lm -pthread -O3 -Wall -funroll-loops    # -march=native, -Wno-unused-result 제거\n",
    "```\n",
    "\n",
    "* 소스코드 수정\n",
    "\n",
    "```\n",
    "$ grep malloc.h *.c\n",
    "compute-accuracy.c:#include <malloc.h>\n",
    "distance.c:#include <malloc.h>\n",
    "word-analogy.c:#include <malloc.h>\n",
    "```\n",
    "malloc.h를 include하는 코드를 모두 지워버립니다. 혹시 아까우시다면 주석 처리를 해도 괜찮아요. ^^;\n",
    "\n",
    "\n",
    "* 빌드\n",
    "\n",
    "```\n",
    "$ make\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 영어 코퍼스로 맛보기\n",
    "----\n",
    "\n",
    "* 샘플 코퍼스 다운로드\n",
    "```\n",
    "$ wget http://mattmahoney.net/dc/text8.zip\n",
    "$ unzip text8.zip\n",
    "```\n",
    "\n",
    "* word vector 생성\n",
    "\n",
    "```\n",
    "$ ./word2vec -train text8 -output text8.w2v -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 4 -binary 1 -iter 15\n",
    "Starting training using file text8\n",
    "Vocab size: 71291\n",
    "Words in train file: 16718843\n",
    "Alpha: 0.000005  Progress: 100.02%  Words/thread/sec: 92.08k\n",
    "\n",
    "$ ls -l text8.w2v\n",
    "-rw-r--r--  1 jamie  staff  57704809 11  6 01:12 text8.w2v\n",
    "```\n",
    "-threads argument는 CPU 코어의 갯수에 맞게 적절히 넣으면 됩니다.\n",
    "\n",
    "\n",
    "* word analogy 해보기\n",
    "\n",
    "A B C 세 단어를 (소문자로) 넣으면 vector(A) - vector(B) + vector(C)를 계산하여 벡터간 거리로 정렬하여 가장 가까운 단어부터 보여줍니다. (우왕ㅋ굳ㅋ)\n",
    "\n",
    "```\n",
    "$ ./word-analogy text8.w2v\n",
    "Enter three words (EXIT to break): athens greece baghdad\n",
    "\n",
    "Word: athens  Position in vocabulary: 3068\n",
    "\n",
    "Word: greece  Position in vocabulary: 1248\n",
    "\n",
    "Word: baghdad  Position in vocabulary: 7173\n",
    "\n",
    "                                              Word              Distance\n",
    "------------------------------------------------------------------------\n",
    "                                              iran\t\t0.490723\n",
    "                                             syria\t\t0.488516\n",
    "                                             egypt\t\t0.458451\n",
    "                                            turkey\t\t0.450124\n",
    "                                             basra\t\t0.448245\n",
    "                                              iraq\t\t0.444451\n",
    "                                        sassanians\t\t0.438153\n",
    "                                             mosul\t\t0.431619\n",
    "                                           lebanon\t\t0.428784\n",
    "                                         kurdistan\t\t0.426602\n",
    "                                              sadr\t\t0.423816\n",
    "                                            persia\t\t0.423356\n",
    "                                             konya\t\t0.421188\n",
    "                                           abbasid\t\t0.415041\n",
    "                                            khitan\t\t0.410358\n",
    "                                           bukhara\t\t0.410169\n",
    "                                            kirkuk\t\t0.405520\n",
    "                                           fatimid\t\t0.404706\n",
    "                                           iranian\t\t0.404393\n",
    "                                           ayyubid\t\t0.404103\n",
    "                                           mameluk\t\t0.403826\n",
    "                                             libya\t\t0.400661\n",
    "                                           almohad\t\t0.400090\n",
    "                                          pakistan\t\t0.398301\n",
    "                                          iranians\t\t0.397244\n",
    "                                              arab\t\t0.395184\n",
    "                                              zmir\t\t0.394183\n",
    "                                            zagros\t\t0.394126\n",
    "                                            mashal\t\t0.393248\n",
    "                                             balkh\t\t0.393002\n",
    "                                       afghanistan\t\t0.391483\n",
    "                                           tunisia\t\t0.389862\n",
    "                                           tahmasp\t\t0.389630\n",
    "                                        uzbekistan\t\t0.389564\n",
    "                                            tigris\t\t0.389527\n",
    "                                        tajikistan\t\t0.389425\n",
    "                                           emirate\t\t0.388417\n",
    "                                        kermanshah\t\t0.388197\n",
    "                                           khanate\t\t0.385721\n",
    "                                             hejaz\t\t0.384893\n",
    "```\n",
    "\n",
    "* 샘플 word analogy 쿼리\n",
    "\n",
    "```\n",
    "$ grep -A1 \"^:\" questions-words.txt\n",
    ": capital-common-countries\n",
    "Athens Greece Baghdad Iraq\n",
    "--\n",
    ": capital-world\n",
    "Abuja Nigeria Accra Ghana\n",
    "--\n",
    ": currency\n",
    "Algeria dinar Angola kwanza\n",
    "--\n",
    ": city-in-state\n",
    "Chicago Illinois Houston Texas\n",
    "--\n",
    ": family\n",
    "boy girl brother sister\n",
    "--\n",
    ": gram1-adjective-to-adverb\n",
    "amazing amazingly apparent apparently\n",
    "--\n",
    ": gram2-opposite\n",
    "acceptable unacceptable aware unaware\n",
    "--\n",
    ": gram3-comparative\n",
    "bad worse big bigger\n",
    "--\n",
    ": gram4-superlative\n",
    "bad worst big biggest\n",
    "--\n",
    ": gram5-present-participle\n",
    "code coding dance dancing\n",
    "--\n",
    ": gram6-nationality-adjective\n",
    "Albania Albanian Argentina Argentinean\n",
    "--\n",
    ": gram7-past-tense\n",
    "dancing danced decreasing decreased\n",
    "--\n",
    ": gram8-plural\n",
    "banana bananas bird birds\n",
    "--\n",
    ": gram9-plural-verbs\n",
    "decrease decreases describe describes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "4. gensim으로 요리하기\n",
    "----\n",
    "\n",
    "\n",
    "### gensim 설치\n",
    "\n",
    "```\n",
    "$ pip install gensim\n",
    "```\n",
    "끝 (우왕ㅋ굳ㅋ)\n",
    "\n",
    "* 참고\n",
    "  - https://radimrehurek.com/gensim/models/word2vec.html\n",
    "  - http://rare-technologies.com/word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### word2vec 모델을 만들어 봅시다.\n",
    "\n",
    "import gensim\n",
    "\n",
    "sentences = sum([line.split() for line in open('text8')], [])    # text8 코퍼스를 모두 읽어들입니다.\n",
    "print sentences[:10]\n",
    "\n",
    "# C 프로그램과 유사하게 학습을 실행합니다.\n",
    "gensim_model = gensim.models.Word2Vec(sentences, size=200, window=8, min_count=5, workers=4, iter=15)\n",
    "\n",
    "gensim_model.save('text8.w2v.gensim')    # 학습한 모델을 저장합니다.\n",
    "gensim_new_model = gensim.models.Word2Vec.load('text8.w2v.gensim')    # 저장한 모델을 읽어들입니다.\n",
    "\n",
    "# 하지만 오래 살기 위해 이러지 말고 Jeff Dean을 믿고 씁시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* woman - man + king = [(u'queen', 0.5442705750465393), (u'anjou', 0.48470211029052734), (u'mistress', 0.4786876440048218)]\n",
      "\n",
      "* The lone duckling = cereal\n",
      "\n",
      "* Similarity between woman and man = 0.609136053228\n",
      "\n",
      "* Vector of man:\n",
      "[  7.33143315e-02   4.47705248e-03  -1.42499268e-01  -2.31839549e-02\n",
      "   5.60476705e-02   7.34771714e-02  -1.50984228e-02   1.18450247e-01\n",
      "  -6.05627187e-02   1.03504187e-03  -7.04200566e-03  -2.98108961e-02\n",
      "   4.88713607e-02   3.99061739e-02   5.11737652e-02   1.13909893e-01\n",
      "  -5.65241128e-02   1.44870996e-01  -6.46180287e-02   2.84903478e-02\n",
      "  -1.73272347e-04   2.16623698e-03  -4.42492999e-02   2.15424523e-02\n",
      "  -9.79404822e-02  -5.70471734e-02  -8.38414803e-02  -3.98078337e-02\n",
      "  -2.82810424e-02  -5.97659014e-02  -8.99010152e-02  -4.27372977e-02\n",
      "   1.37358561e-01  -1.16520800e-01  -2.86092572e-02   9.05053783e-03\n",
      "  -1.10818781e-01   6.26738071e-02  -8.74709710e-03  -6.23363666e-02\n",
      "   3.06309406e-02   7.23769069e-02   2.64032315e-02  -3.64326164e-02\n",
      "   2.27875058e-02   3.51437218e-02  -2.21713278e-02   6.35321438e-02\n",
      "   3.92928645e-02   6.72468841e-02   6.16236180e-02   1.60994977e-02\n",
      "   3.68421040e-02  -7.57057685e-04   1.43400533e-02   2.08057109e-02\n",
      "  -2.56620031e-02  -1.74928214e-02  -6.79451078e-02   2.28550389e-01\n",
      "   2.95848306e-02  -4.95836437e-02  -9.28906277e-02  -1.31258611e-02\n",
      "   8.44271258e-02  -5.85108213e-02   1.27610892e-01  -1.61300272e-01\n",
      "  -3.86805683e-02   9.40249413e-02  -7.97226802e-02   1.73402224e-02\n",
      "  -1.35863349e-01  -2.59825913e-03   1.63272955e-02  -3.70907262e-02\n",
      "  -3.47102396e-02  -5.45544736e-02  -5.59276603e-02  -2.62273699e-02\n",
      "   8.36422145e-02  -4.78602126e-02  -5.06647229e-02  -3.56900431e-02\n",
      "  -1.54472515e-01   1.60729244e-01   2.95889657e-02  -5.77965677e-02\n",
      "   2.50005797e-02   1.01126358e-01   7.09445542e-03  -9.81054828e-03\n",
      "   4.08322625e-02   2.99320221e-02  -2.28108302e-01  -8.27506706e-02\n",
      "   7.86937773e-02  -2.16140836e-01  -2.10007727e-02  -1.22699238e-01\n",
      "  -1.05361030e-01   1.06156856e-01  -7.50449896e-02  -1.77446082e-01\n",
      "  -5.01543237e-03  -2.06043152e-03  -1.21583804e-01   4.49838862e-02\n",
      "  -7.12453648e-02   9.49681154e-04   9.16545913e-02  -5.26720136e-02\n",
      "   7.56991059e-02  -3.68758664e-02  -1.17526529e-02   1.12534035e-02\n",
      "  -8.22386220e-02  -1.47435650e-01   8.55840091e-03   3.14476117e-02\n",
      "   3.07784807e-02  -7.35500753e-02  -1.12056829e-01   3.51802469e-03\n",
      "  -3.60754021e-02  -5.96918212e-03  -2.73477635e-03   3.20442840e-02\n",
      "   7.77328163e-02   2.55079325e-02  -1.21430680e-02   6.83629885e-03\n",
      "   5.69066145e-02  -2.46821474e-02  -1.37283489e-01   5.47250807e-02\n",
      "   1.74730103e-02   5.91400191e-02  -2.38274895e-02   4.75048088e-02\n",
      "  -1.58687130e-01  -3.62710953e-02  -9.34972893e-03   3.02046202e-02\n",
      "   8.13980028e-02  -7.08405972e-02   4.49590646e-02   5.10611162e-02\n",
      "   5.01586385e-02   9.83890444e-02   5.56760505e-02   8.56964886e-02\n",
      "  -3.76485959e-02   3.78444791e-02  -8.81629586e-02   4.42646332e-02\n",
      "  -4.44218740e-02  -1.02799371e-01   4.58603986e-02   7.39911646e-02\n",
      "   6.97595300e-03  -1.47992112e-02  -3.75589402e-03  -1.91003513e-02\n",
      "   7.90821090e-02  -7.68974572e-02   1.85665535e-03  -1.13060862e-01\n",
      "   1.56092346e-02   1.34267667e-02  -3.72457393e-02   7.13540614e-02\n",
      "  -4.00832742e-02  -4.34733965e-02  -3.81085612e-02   6.78443313e-02\n",
      "   1.23916700e-01   3.29217538e-02  -6.80417195e-02  -3.48744467e-02\n",
      "   1.06518073e-02   8.56158957e-02   2.03335173e-02   1.55982953e-02\n",
      "   1.81939872e-03  -7.73527995e-02   8.82450044e-02   1.59542542e-02\n",
      "   5.55888079e-02  -3.17336321e-02  -3.56733054e-02   2.60719340e-02\n",
      "   8.08634236e-02   3.29484753e-02   4.00330648e-02  -8.20988640e-02\n",
      "  -2.72722747e-02   6.89013954e-03   5.52785695e-02   4.57945727e-02]\n"
     ]
    }
   ],
   "source": [
    "### 기존에 C 프로그램으로 학습한 모델을 읽어들여 여러가지 연산을 해봅니다.\n",
    "\n",
    "import gensim\n",
    "\n",
    "gensim_model = gensim.models.Word2Vec.load_word2vec_format('text8.w2v', binary=True)    # 모델을 읽어들입니다.\n",
    "\n",
    "# word analogy\n",
    "print '* woman - man + king =', gensim_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=3)\n",
    "print\n",
    "\n",
    "# 미운 오리 새끼를 찾습니다. ;)\n",
    "print '* The lone duckling =', gensim_model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
    "print\n",
    "\n",
    "# 두 단어의 유사도를 계산합니다.\n",
    "print '* Similarity between woman and man =', gensim_model.similarity('woman', 'man')\n",
    "print\n",
    "\n",
    "# 단어의 벡터를 출력합니다.\n",
    "print '* Vector of man:'\n",
    "print gensim_model['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. KoNLPy 설치\n",
    "----\n",
    "\n",
    "### konlpy 패키지 설치\n",
    "\n",
    "```\n",
    "$ pip install konlpy\n",
    "```\n",
    "\n",
    "### mecab-ko 프로그램 설치\n",
    "\n",
    "* 아래와 같은 마법의 스크립트는 에러가 나니 한땀한땀 해보겠습니다.\n",
    "\n",
    "```\n",
    "$ bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "```\n",
    "\n",
    "* \\$HOME/usr/mecab-ko 아래에 설치한다고 가정하고 아래와 같이 소스코드를 다운로드합니다.\n",
    "\n",
    "```\n",
    "$ cd $HOME/usr\n",
    "$ wget https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
    "```\n",
    "\n",
    "* 압축을 풀고 빌드/설치를 합니다.\n",
    "\n",
    "```\n",
    "$ tar xfz mecab-0.996-ko-0.9.2.tar.gz\n",
    "$ cd mecab-0.996-ko-0.9.2\n",
    "$ ./configure --prefix=$HOME/usr/mecab-ko\n",
    "$ make && make install\n",
    "```\n",
    "\n",
    "### mecab-ko 사전 설치\n",
    "\n",
    "* 역시 \\$HOME/usr/mecab-ko 아래에 설치한다고 가정하고 아래와 같이 사전을 다운로드합니다.\n",
    "\n",
    "```\n",
    "$ cd $HOME/usr\n",
    "$ wget https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz\n",
    "```\n",
    "\n",
    "* 압축을 풀고 빌드/설치를 합니다.\n",
    "\n",
    "```\n",
    "$ tar xfz mecab-ko-dic-2.0.1-20150920.tar.gz\n",
    "$ cd mecab-ko-dic-2.0.1-20150920\n",
    "$ ./autogen.sh\n",
    "$ ./configure --prefix=$HOME/usr/mecab-ko --with-mecab-config=$HOME/usr/mecab-ko/bin/mecab-config\n",
    "$ make && make install\n",
    "```\n",
    "\n",
    "### 환경 설정\n",
    "\n",
    "```\n",
    "$ export LD_LIBRARY_PATH=$HOME/usr/mecab-ko/lib:$LD_LIBRARY_PATH    # Linux의 경우\n",
    "$ export DYLD_LIBRARY_PATH=$HOME/usr/mecab-ko/lib:$DYLD_LIBRARY_PATH    # Mac의 경우\n",
    "```\n",
    "\n",
    "### 테스트\n",
    "\n",
    "```\n",
    "$ cd $HOME/usr/mecab-ko\n",
    "$ bin/mecab\n",
    "존나좋군?\n",
    "존나\tMAG,*,F,존나,*,*,*,*\n",
    "좋\tVA,*,T,좋,*,*,*,*\n",
    "군\tEF,*,T,군,*,*,*,*\n",
    "?\tSF,*,*,*,*,*,*,*\n",
    "EOS\n",
    "```\n",
    "![](http://pds23.egloos.com/pds/201112/24/41/d0021441_4ef5e7b6eb457.jpg)\n",
    "\n",
    "### mecab-ko python 모듈 설치\n",
    "\n",
    "* 소스 코드를 다운로드 합니다.\n",
    "\n",
    "```\n",
    "$ cd $HOME/usr\n",
    "$ git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
    "```\n",
    "\n",
    "* 설치 및 빌드를 합니다.\n",
    "\n",
    "```\n",
    "$ cd mecab-python-0.996\n",
    "$ python setup.py build\n",
    "$ sudo python setup.py install    # virtualenv를 사용하신다면 sudo는 내려놓으셔도 됩니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(존나, MAG),\n",
      " (좋, VA),\n",
      " (군, EF),\n",
      " (?, SF)]\n"
     ]
    }
   ],
   "source": [
    "### KoNLPy에서 mecab-ko를 사용해 봅시다.\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "import os\n",
    "\n",
    "# 사전 경로를 주지 않으면 /usr/local/lib/mecab/dic/mecab-ko-dic을 찾습니다.\n",
    "mecab = Mecab('%s/usr/mecab-ko/lib/mecab/dic/mecab-ko-dic' % os.environ['HOME'])\n",
    "pprint(mecab.pos(u'존나좋군?'))    # [(형태소1, 태그1), (형태소2, 태그2), ...)] 튜플의 리스트 형태로 리턴합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 한국어 코퍼스로 word2vec 생성\n",
    "----\n",
    "\n",
    "### 원시 코퍼스\n",
    "\n",
    "* 원시 코퍼스의 압축을 푼다.\n",
    "\n",
    "```\n",
    "$ cat raw_corpus.zip.0 raw_corpus.zip.1 >> raw_corpus.zip\n",
    "$ unzip -q raw_corpus.zip\n",
    "```\n",
    "\n",
    "* 인코딩을 UTF-16에서 UTF-8로 변환하여 하나의 파일로 합친다. => raw_corpus.xml\n",
    "\n",
    "```\n",
    "$ iconv -f UTF-16 -t UTF-8 raw_corpus/*.txt > raw_corpus.xml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 태그를 제거하고 문장만 남긴다. => raw_corpus.txt\n",
    "\n",
    "def strip_tag(line, tag):\n",
    "    \"\"\"\n",
    "    라인을 둘러싼 열고 닫는 xml 태그를 제거하는 함수\n",
    "    @param  line  xml 태그를 포함하는 라인\n",
    "    @param  tag   태그명\n",
    "    @return       태그가 제거된 라인\n",
    "    \"\"\"\n",
    "    open_tag = '<%s>' % tag\n",
    "    close_tag = '</%s>' % tag\n",
    "    stripped = line\n",
    "    if stripped.startswith(open_tag):\n",
    "        stripped = stripped[len(open_tag):]\n",
    "    if stripped.endswith(close_tag):\n",
    "        stripped = stripped[:-len(close_tag)]\n",
    "    return stripped.strip()\n",
    "\n",
    "with open('raw_corpus.txt', 'w') as fout:\n",
    "    for line in open('raw_corpus.xml'):    # 앞서 xml로 저장한 원시 코퍼스를 열어서\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith('<head>') or line.startswith('<p>'):    # <head>, <p> 태그로 시작하는 문장만\n",
    "            line = strip_tag(line, 'head')\n",
    "            line = strip_tag(line, 'p')\n",
    "            if line:\n",
    "                print >> fout, line    # 한 줄에 한 문장씩 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### KoNLPy를 이용해 품사 태깅을 수행한다. => raw_corpus.tag\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.utils import pprint\n",
    "import os\n",
    "\n",
    "mecab = Mecab('%s/usr/mecab-ko/lib/mecab/dic/mecab-ko-dic' % os.environ['HOME'])\n",
    "\n",
    "with open('raw_corpus.tag', 'w') as fout:\n",
    "    for line in open('raw_corpus.txt'):    # 앞서 txt로 저장한 원시 코퍼스를 열어서\n",
    "        for morph, tag in mecab.pos(unicode(line, 'UTF-8')):    # 형태소 분석을 수행한 다음\n",
    "            print >> fout, '%s/%s' % (morph.encode('UTF-8'), tag.encode('UTF-8')),    # '정치/NNG'의 형태로\n",
    "        print >> fout    # 한 줄에 한 문장씩 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앞서 생성한 자동 태깅한 코퍼스를 이용하여 '단어/태그'의 벡터를 생성한다. => raw_corpus.w2v\n",
    "\n",
    "```\n",
    "$ ./word2vec -train raw_corpus.tag -output raw_corpus.w2v -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 4 -binary 1 -iter 15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 의미 코퍼스로 학습 집합 생성\n",
    "----\n",
    "\n",
    "### 의미 코퍼스\n",
    "\n",
    "* 의미 코퍼스의 압축을 푼다.\n",
    "\n",
    "```\n",
    "$ cat sense_corpus.zip.0 sense_corpus.zip.1 >> sense_corpus.zip\n",
    "$ unzip -q sense_corpus.zip\n",
    "```\n",
    "\n",
    "* 인코딩을 UTF-16에서 UTF-8로 변환하여 하나의 파일로 합친다. => sense_corpus.xml\n",
    "\n",
    "```\n",
    "$ iconv -f UTF-16 -t UTF-8 sense_corpus/*.txt > sense_corpus.xml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 특정 단어를 포함하는 문장을 추출하여 단어의 의미 번호와 함께 출력한다. => sense_corpus.tag\n",
    "\n",
    "TARGET = '연기__'    # 추출을 원하는 단어. 예: 배, 경기, 연기\n",
    "\n",
    "import re\n",
    "\n",
    "LINE_ID_PTN = re.compile('[0-9A-Z_]{4}\\\\d{4}-\\\\d{7,8}')    # 형태소 분석 된 어절 한 줄의 맨 앞에 ID 패턴\n",
    "\n",
    "with open('sense_corpus.tag', 'w') as fout:\n",
    "    sent = []\n",
    "    for line in open('sense_corpus.xml'):    # 앞서 xml로 저장한 의미 코퍼스를 열어서\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if LINE_ID_PTN.match(line):    # 다른 라인은 버리고 어절만을 취하여\n",
    "            _, morphs = line.rsplit('\\t', 1)\n",
    "            sent.extend(morphs.split(' + '))\n",
    "        else:\n",
    "            # 한 문장에서 원하는 단어의 의미 번호를 모두 추출하여\n",
    "            sense_nums = set([morph[len(TARGET):morph.rindex('/')] for morph in sent if morph.startswith(TARGET)])\n",
    "            if len(sense_nums) == 1:    # 한 문장에 원하는 단어의 의미가 한가지로만 쓰인 문장만을 골라서\n",
    "                try:\n",
    "                    sense_num = int(list(sense_nums)[0])\n",
    "                    if sense_num < 50:    # 의미 번호가 너무 크면 뭔가 에러가 있으므로 버리고\n",
    "                        # 단어에 붙어있는 모든 의미 번호를 제거하고\n",
    "                        no_sense_tag = [re.sub(r\"\"\"__x?\\d\\d/\"\"\", '/', morph) for morph in sent]\n",
    "                        # 문장 맨 앞에 원하는 단어의 의미 번호를 함께 출력\n",
    "                        print >> fout, '%d\\t%s' % (sense_num, ' '.join(no_sense_tag))\n",
    "                except ValueError:\n",
    "                    # 간혹 의미 번호가 '연기__x01'과 같이 특수하게 부여된 것이 있는데, 이런건 버린다.\n",
    "                    pass\n",
    "            sent = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "8. 단어 의미 중의성 해결 - Part 1 - 전통적인 방법\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### 의미 코퍼스를 읽어들입니다.\n",
    "\n",
    "sense_corpus = []\n",
    "for line in open('sense_corpus.tag'):\n",
    "    sense, features = line.split('\\t', 1)\n",
    "    sense_corpus.append((int(sense), features.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vocabulary: 7628\n"
     ]
    }
   ],
   "source": [
    "### 전부 바이너리 벡터로 표현합니다.\n",
    "\n",
    "import numpy as npy\n",
    "\n",
    "def to_bin_vector(vocabulary, features, trg):\n",
    "    \"\"\"\n",
    "    vocabulary 사전을 이용하여 모든 자질을 하나의 bag-of-words 벡터로 표현하는 함수\n",
    "    @param  vocabulary  vocabulary 사전\n",
    "    @param  features    자질 목록\n",
    "    @param  trg         중의성을 해소할 단어\n",
    "    @return             벡터\n",
    "    \"\"\"\n",
    "    vec_sum = npy.zeros([len(vocabulary),], dtype=npy.int8)    # vocabulary 크기 만큼 벡터를 0으로 초기화 합니다.\n",
    "    for feature in features:\n",
    "        vec_sum[vocabulary[feature]] = 1    # 자질 번호에 해당하는 벡터 값을 1로 설정해 줍니다.\n",
    "    return vec_sum\n",
    "\n",
    "all_features = set(sum([features for _, features in sense_corpus], []))    # 모든 자질을 합칩니다.\n",
    "vocabulary = {feature: num for num, feature in enumerate(all_features)}    # 각 자질에 고유 번호를 부여하여 사전을 구축합니다.\n",
    "print 'Number of vocabulary:', len(vocabulary)\n",
    "\n",
    "# 자질을 전부 벡터로 표현합니다.\n",
    "bin_train = [(sense, to_bin_vector(vocabulary, features, '연기')) for sense, features in sense_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### SVM을 이용하여 학습 및 평가를 수행하는 함수를 작성합니다.\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "\n",
    "def evaluate(X, Y):\n",
    "    \"\"\"\n",
    "    SVM을 이용하여 평가를 수행하는 함수\n",
    "    @param  X  자질 벡터\n",
    "    @param  Y  outcome\n",
    "    @return    정확도\n",
    "    \"\"\"\n",
    "    # 학습 코퍼스를 8:2의 비율로 학습/평가 코퍼스로 나눕니다.\n",
    "    X_train, X_eval, y_train, y_eval = cross_validation.train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    svm_model = svm.SVC()\n",
    "    svm_model.fit(X_train, y_train)    # 학습 집합을 이용해 학습합니다.\n",
    "    return svm_model.score(X_eval, y_eval)    # 평가 집합으로 정확도를 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39393939393939392"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 평가를 해봅니다.\n",
    "\n",
    "evaluate([features for _, features in bin_train], [sense for sense, _ in bin_train])\n",
    "\n",
    "# oTL ㅠ.ㅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pbs.twimg.com/profile_images/2104851627/image.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 단어 의미 중의성 해결 - Part 2 - word2vec 활용\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 200\n"
     ]
    }
   ],
   "source": [
    "### 전부 word2vec을 이용하여 벡터로 표현합니다.\n",
    "\n",
    "import gensim\n",
    "import numpy as npy\n",
    "\n",
    "def to_w2v_vector(model, features, trg):\n",
    "    \"\"\"\n",
    "    word2vec 모델을 이용하여 모든 자질을 더해서 하나의 벡터로 표현하는 함수\n",
    "    @param  model     word2vec 모델\n",
    "    @param  features  자질 목록\n",
    "    @param  trg       중의성을 해소할 단어\n",
    "    @return           벡터\n",
    "    \"\"\"\n",
    "    vec_sum = npy.zeros([model.vector_size,])    # word2vec 모델의 벡터 크기 만큼 벡터를 0으로 초기화 합니다.\n",
    "    for feature in set(features):\n",
    "        try:\n",
    "            vec_sum += model[feature]\n",
    "        except KeyError:\n",
    "            pass    # Out-Of-Vocabulary 자질들은 그냥 넘어갑니다.\n",
    "    return vec_sum\n",
    "\n",
    "# 원시 코퍼스로 pre-training 해둔 word2vec 모델을 읽어들입니다.\n",
    "w2v_model = gensim.models.Word2Vec.load_word2vec_format('raw_corpus.w2v', binary=True)\n",
    "print 'Vector size:', w2v_model.vector_size\n",
    "\n",
    "# 자질을 전부 벡터로 표현합니다.\n",
    "w2v_train = [(sense, to_w2v_vector(w2v_model, features, '연기')) for sense, features in sense_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43097643097643096"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 평가를 해봅니다.\n",
    "\n",
    "evaluate([features for _, features in w2v_train], [sense for sense, _ in w2v_train])\n",
    "\n",
    "# Part 1보다는 좀 낫지만 여전히 oTL ㅠ.ㅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](https://cdn.mirror.wiki/http://cdn.mirror.wiki/https://attachment.namu.wiki/otl.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
